python3 preprocess.py \
-train_src data/src-train.txt \
-train_tgt data/tgt-train.txt \
-valid_src data/src-val.txt \
-valid_tgt data/tgt-val.txt \
-save_data data/demo \
-lower

python embeddings_to_torch.py \
-emb_file_both "../data/glove.840B.300d.txt" \
-dict_file "data/demo.vocab.pt" \
-output_file "data/embeddings"

python3 train.py \
-data data/demo \
-save_model demo-model \
-batch_size 16 -valid_batch_size 16 \
-encoder_type brnn -decoder_type rnn -layer 2 \
-rnn_size 600 \
-optim adam -learning_rate 0.001 -learning_rate_decay 1.0 -start_decay_steps 10000000 \
-save_checkpoint_steps 200 \
-gpu_ranks 3 -world_size 1 \
-log_file log.txt



python translate.py \
-model demo-model_acc_XX.XX_ppl_XXX.XX_eX.pt \
-src data/src-test.txt \
-output pred.txt \
-replace_unk -verbose


preprocess
#小文字化
--lower True
#文の長さ、要指定
--src_seq_length 50
--tgt_seq_length 50

train
#エンコーダーのタイプ。デフォルトはrnnだが双方向にするべき
--encoder_type brnn
#層の数。デフォルトは2
--layer
#RNNのタイプ。デフォルトはLSTM、要検討
--rnn_type GRU
#gpuの番号
--gpu_ranks
#学習済み単語ベクトルの更新
--pre_word_vecs_enc
--pre_word_vecs_dec
#学習ずみ単語ベクトルの更新
--fix_word_vecs_enc
#バッチサイズ
--batch_size 64 -> 32
#optimizer
--optim
#学習率
-learning_rate
#学習率減衰,adamなら不要
-learning_rate_decay
--gpu_ranks
--world_size
#隠れ状態のサイズ
--rnn_size 500
#ログファイルのパス
-log_file
#評価のステップ
-valid_steps 10000
#保存のステップ
-checkpoint_steps 5000
#計算結果のレポートの間隔
-report_every 50
#テストの間隔
-valid_steps 10000 -> 2000
