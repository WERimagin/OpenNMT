python3 preprocess.py \
-train_src data/squad-src-train.txt \
-train_tgt data/squad-tgt-train.txt \
-valid_src data/squad-src-val.txt \
-valid_tgt data/squad-tgt-val.txt \
-save_data data/demo \
-lower

python embeddings_to_torch.py \
-emb_file_both "../data/glove.840B.300d.txt" \
-dict_file "data/demo.vocab.pt" \
-output_file "data/embeddings"

python3 train.py \
-data data/demo \
-save_model model_data/demo-model \
-pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
-gpu_ranks 3 -world_size 1 \
-log_file "log.txt"


python translate.py \
-model model_data/demo-model_step_10500.pt \
-src data/short_fra_val.txt \
-output pred.txt \
-replace_unk -verbose


python translate.py \
-model model_data/demo-model_step_7000.pt \
-src data/squad-src-val.txt \
-output pred.txt \
-replace_unk -verbose

python bleu.py \
-src data/squad-src-val.txt \
-tgt data/squad-tgt-val.txt \
-pred pred.txt

preprocess
#小文字化
--lower True
#文の長さ、要指定
--src_seq_length 50
--tgt_seq_length 50

train
#エンコーダーのタイプ。デフォルトはrnnだが双方向にするべき
--encoder_type brnn
#層の数。デフォルトは2
--layer
#RNNのタイプ。デフォルトはLSTM、要検討
--rnn_type GRU
#gpuの番号
--gpu_ranks
#単語埋め込みサイズ
-word_vec_size 500
#学習済み単語ベクトルのパス
--pre_word_vecs_enc
--pre_word_vecs_dec
#学習ずみ単語ベクトルの更新
--fix_word_vecs_enc
#バッチサイズ
--batch_size 64 -> 32
#optimizer
--optim
#学習率
-learning_rate
#学習率減衰,adamなら不要
-learning_rate_decay
--gpu_ranks
--world_size
#隠れ状態のサイズ
--rnn_size 500
#ログファイルのパス
-log_file
#評価のステップ
-valid_steps 10000
#保存のステップ
-checkpoint_steps 5000
#計算結果のレポートの間隔
-report_every 50
#テストの間隔
-valid_steps 10000 -> 2000
#テスト時に、UNKを最もスコアの高いもので置換
-replace_unk
#テスト時に、それぞれの文につきスコアを表示
-verbose
