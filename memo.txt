python preprocess.py \
-train_src data/squad-src-train-interro.txt \
-train_tgt data/squad-tgt-train-interro.txt \
-valid_src data/squad-src-val-interro.txt \
-valid_tgt data/squad-tgt-val-interro.txt \
-save_data data/demo \
-lower -dynamic_dict

python preprocess.py \
-train_src data/squad-src-train-du.txt \
-train_tgt data/squad-tgt-train-du.txt \
-valid_src data/squad-src-val-du.txt \
-valid_tgt data/squad-tgt-val-du.txt \
-save_data data/demo \
-lower -dynamic_dict

python embeddings_to_torch.py \
-emb_file_both "../data/glove.840B.300d.txt" \
-dict_file "data/demo.vocab.pt" \
-output_file "data/embeddings"

python3 train.py \
-data data/demo \
-save_model model_data/demo-model \
-pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
-gpu_ranks 3 -world_size 1 \
-copy_attn -copy_attn_type general \
-coverage_attn -lambda_coverage 1 \
-share_embeddings -share_decoder_embeddings \


python3 train.py \
-data data/demo \
-save_model model_data/demo-model \
-pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
-copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
-gpu_ranks 3 -world_size 1 \
-optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

python3 train.py \
-data data/demo \
-save_model model_data/demo-model \
-pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
-gpu_ranks 3 -world_size 1 \
-optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

#transformer
python train.py \
-data data/demo -save_model model_data/demo-model \
-layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8  \
-encoder_type transformer -decoder_type transformer -position_encoding \
-train_steps 200000  -max_generator_batches 2 -dropout 0.1 \
-batch_size 4096 -batch_type tokens -normalization tokens  -accum_count 2 \
-optim adam -learning_rate 2 -adam_beta2 0.998 -decay_method noam  \
-max_grad_norm 0 -param_init 0  -param_init_glorot \
-label_smoothing 0.1 -valid_steps 5000 -save_checkpoint_steps 5000 -warmup_steps 8000 \
-world_size 1 -gpu_ranks 3

python translate.py \
-src data/squad-src-val-interro.txt \
-output pred.txt \
-replace_unk -dynamic_dict \
-model model_data/

python bleu.py \
-src data/squad-src-val-nonshuffle.txt \
-tgt data/squad-tgt-val-nonshuffle.txt \
-pred pred.txt

python bleu.py \
-src data/squad-src-val-interro.txt \
-tgt data/squad-tgt-val-interro.txt \
-pred data/pred.txt

python bleu.py \
-src data/coqa-src-dev.txt \
-tgt data/coqa-src-dev.txt \
-pred pred.txt

#bleuスコア
python bleu.py \
-src data/squad-src-val-interro.txt \
-tgt data/squad-tgt-val-interro.txt \
-pred data/squad-pred-val-interro.txt

#疑問詞をのぞいたbleuスコア
python bleu_noninterro.py \
-src data/squad-src-val-nonshuffle.txt \
-tgt data/squad-tgt-val-nonshuffle.txt \
-pred data/squad-pred-val-nonshuffle.txt \
-noninterro data/squad-noninterro-val-interro.txt -ratio 0.01





python2 qgevalcap/eval.py \
-src data/squad-src-val-overlap-noninterro.txt \
-tgt data/squad-tgt-val-overlap-noninterro.txt \
-out squad-pred-val-overlap-noninterro.txt

python2 qgevalcap/eval.py \
-src data/squad-src-val-du.txt \
-tgt data/squad-tgt-val-du.txt \
-out squad-pred-val-du.txt

preprocess
  #小文字化
  --lower True
  #文の長さ、要指定
  --src_seq_length 50
  --tgt_seq_length 50
  #単語辞書の共有
  -share_vocab
  #コピーに必要
  -dynamic_dict

train
  #単語ベクトル
    #単語埋め込みサイズ
    -word_vec_size 500
    #学習済み単語ベクトルのパス
    --pre_word_vecs_enc
    --pre_word_vecs_dec
    #学習ずみ単語ベクトルの固定
    --fix_word_vecs_enc
    #エンコーダーとデコーダーで単語ベクトルの共有
    -share_embeddings
    #デコーダーのinputとoutputの共有。これには単語ベクトルサイズと隠れ状態サイズが同じである必要がある
    -share_decoder_embeddings
  #モデルの設定
    #エンコーダーのタイプ。デフォルトはrnnだが双方向にするべき
    --encoder_type brnn
    #層の数。デフォルトは2
    --layer
    #隠れ状態のサイズ
    --rnn_size 500
    #RNNのタイプ。デフォルトはLSTM、要検討
    --rnn_type GRU
    #encoderの最終隠れ状態をdecoderに渡す前の層を入れるかどうか
    -bridge
  #データの設定
    #バッチサイズ
    --batch_size 64 -> 32
  #gpu
    #gpuの番号
    --gpu_ranks
    #gpuの数
    -world_size
  #最適化機
    #optimizer
    --optim
    #学習率
    -learning_rate
    #学習率減衰,adamなら不要
    -learning_rate_decay
    --gpu_ranks
    --world_size
  コピー機構
    #copy-attnを使用するかとその種類
    -copy_attn
    -copy_attn_type None -> general
    #sentenceについて辞書の作成。コピー機構に必要
    -dynamic_dict
  カバレッジ
    #coverageとラムダの値
    -coverage_attn
    -lambda_coverage 1(論文の値)
  #ログファイルのパス
  -log_file
  #評価のステップ
  -valid_steps 10000
  #保存のステップ
  -checkpoint_steps 5000
  #計算結果のレポートの間隔
  -report_every 50
  #テストの間隔
  -valid_steps 10000 -> 2000
  #データのシャッフル
  -shuffle 0->1

#テスト

#テスト時に、UNKを最もスコアの高いもので置換
-replace_unk
#テスト時に、それぞれの文につきスコアを表示
-verbose
#beam
--beam_size 5
#n-bestの生成
-n_best 1
#n-bestの保存ログ
--dump_beam
dump_beam
#bleuの報告
-report_bleu
#コピーに必要
-dynamic_dict
#gpuの番号
-gpu

#最初のウォームアップのステップ
-warmup_steps
#gradの値がこの値を超えたら正規化する
-max_grad_norm
