文そのまま（答えや疑問詞など含めず）
質問文の文字数が5以下のノイズデータは省いたが、それ以外の文は全て使用した。
50文字までを訓練に使用

全ての文
0.40700896414178056
0.23134561150951782
0.15074457485064752
0.10203992991167184

Duらの論文を参考に、内容語の重複がないものを取り除く
疑問詞がないものも取り除く
{
  val:10570
  9737
  train:87599
  78685
  6000
  0.36869636616658863
  0.2063775967570529
  0.133824553207302
  0.09055310119681877
  7000
  0.3640534042527387
  0.2024536444218513
  0.13069366812041094
  0.08922714728875593
}


-overlap:オーバラップしていないものも含む
{
  val:10570
  10024
  train:87599
  81753
  0.35490593637131324
  0.19247020246941443
  0.12223150480960106
  0.08206431976612458
  -beam_size 3
  0.36389284042078746
  0.19556191563829126
  0.12306557484710942
  0.08141793460926491
}

-noninterro:疑問詞がないものも含む
{
  val:10570
  10270
  train:87599
  84365
  9500
  0.3689061536558604
  0.20711415393632365
  0.13572319383764175
  0.09340830219864989
  8500
  0.3605848002215529
  0.20136165336130474
  0.13075139460792767
  0.08934717916889375
  10000
  0.3482898660557566
  0.19104165741696685
  0.12325646936848747
  0.08324030642379629
  13000
  0.3546727016323772
  0.19729206924058848
  0.12703690987263444
  0.08592545606260674
}

コピーとカバレッジの追加
{
  5500
  0.36761840892680653
  0.22192467849212252
  0.154679173629004
  0.11291541501663589
}

duらのデータを使用
{
  5500
  0.37964762979441496
  0.21070869737860146
  0.1360045702569303
  0.09183197851379975
  7000
  0.39143732009863264
  0.22211157468158665
  0.14478863115254467
  0.0987884300592426

  enc_word_size 45000,dec 28000
  word_vec fix
  9000
  0.37964762979441496
  0.21070869737860146
  0.1360045702569303
  0.09183197851379975
}

疑問詞付きで実験
{
  <SEP> + 疑問詞で実験
  inputの辞書サイズは45000,outputは28000
  単語ベクトルは固定
  コピー、カバレッジはなし
  python preprocess.py \
  -train_src data/squad-src-train-interro.txt \
  -train_tgt data/squad-tgt-train.txt \
  -valid_src data/squad-src-val-interro.txt \
  -valid_tgt data/squad-tgt-val.txt \
  -save_data data/demo \
  -lower -src_vocab_size 45000 -tgt_vocab_size 28000

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -gpu_ranks 3 -world_size 1
  12000
  0.42352199549253094
  0.29244977053327637
  0.21881937859302225
  0.16631600370337762
  9000
  0.4365445602660493
  0.30249012971662864
  0.22645192962571192
  0.17202424167198602
}

src_seq_sizeを100に変更し、データ数を増やした
{
  duらのデータを使用
  ここから、src_seq_size 100,tgt_seq_size 50、enc_voc_size 45000,dec_vocab_size 28000に固定
  データサイズ:62000->70000
  python preprocess.py \
  -train_src data/squad-src-train-du.txt \
  -train_tgt data/squad-tgt-train-du.txt \
  -valid_src data/squad-src-val-du.txt \
  -valid_tgt data/squad-tgt-val-du.txt \
  -save_data data/demo \
  -lower

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -gpu_ranks 3 -world_size 1

  12000
  0.3818032773669874
  0.22207359215281677
  0.14836247284785528
  0.1038565408658504
  10000
  0.3797911121140998
  0.21980389442712892
  0.1454957579114939
  0.10071842183474895
}

全てのデータ
{
  overlap+noninterro
  sgdを使用
  python preprocess.py \
  -train_src data/squad-src-train-overlap-noninterro.txt \
  -train_tgt data/squad-tgt-train-overlap-noninterro.txt \
  -valid_src data/squad-src-val-overlap-noninterro.txt \
  -valid_tgt data/squad-tgt-val-overlap-noninterro.txt \
  -save_data data/demo \
  -lower \
  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 10000 -decay_steps 2000

  --beam_size 3
  18000
  0.37134221386631916
  0.2068268052269781
  0.13353502535049772
  0.09116215926940126
  -beam_size 5
  0.35703256254970045
  0.19998870428721507
  0.1303280621388004
  0.08979972747923132
  20500
  0.35238165102010544
  0.19838990179457527
  0.13000550458808965
  0.08987226052408974
}

shuffle
{
  データをpreprocessでシャッフル
  データはoverlap-noninterro
  13000
  0.35449933197783856
  0.19930672435560826
  0.13064106789167962
  0.09046163460412371

  duら
  stepsを25000に
  18000
  0.36857942154151785
  0.21345661038425523
  0.14225483513420595
  0.09894156819452919
  15000
  0.3830418887721869
  0.2242326172442351
  0.15009308209802488
  0.10519081126455221
  13000
  0.38603452563406127
  0.2231796731017263
  0.14824092517212442
  0.10281374896930358

  -start_decay_steps 20000 -decay_steps 2500 -train_steps 50000
  28000
  0.3988480580669166
  0.23560276332121058
  0.1585135752728077
  0.1114058235070068
  37000
  0.39658456174426027
  0.2358171198938562
  0.16000026399319953
  0.11342067478486642
  38000
  0.39549555475888737
  0.2352811643317181
  0.15972696221112345
  0.11339629816619744

  -start_decay_steps 20000 -decay_steps 3000 -train_steps 60000
  30000
  0.400140624537702
  0.23731986280714962
  0.1601483527090614
  0.1127673503484243
  41000
  0.40016893254925556
  0.23866364072141835
  0.16182145550580126
  0.11443930570133387
  qgevalcap/eval.py
  Bleu_1: 0.41161
  Bleu_2: 0.24549
  Bleu_3: 0.16618
  Bleu_4: 0.11771

  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2000 -train_steps 60000
  27000
  0.3935429317536642
  0.23440159835834948
  0.1591811646641791
  0.11273642282453042
  30000
  0.3967899897331695
  0.2362346318980176
  0.1599417471152941
  0.11286157890117823
}

gru
{
  gruを使用、sgdは相性が悪いため、adamを使用
  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -rnn_type GRU \
  -gpu_ranks 3 -world_size 1 \
  -train_steps 30000
  スコアはとても低い
}

bridge
{
  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -gpu_ranks 3 -world_size 1 \
  -bridge \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 3000 -train_steps 50000
  26000
  0.38182877287214717
  0.22205685359382502
  0.14773142620211194
  0.10315436253715742
  44000
  0.3953961968128539
  0.2313639103813019
  0.15466326394909802
  0.1078724355523195
}

コピー
{
  コピーのみを使用
  データはduらのもの
  収束が早い気がするので、-start_decay_stepsを小さくしてもいいきがする。
  python preprocess.py \
  -train_src data/squad-src-train-du.txt \
  -train_tgt data/squad-tgt-train-du.txt \
  -valid_src data/squad-src-val-du.txt \
  -valid_tgt data/squad-tgt-val-du.txt \
  -save_data data/demo \
  -lower -dynamic_dict

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

  python translate.py \
  -src data/squad-src-val-du.txt \
  -output pred.txt \
  -replace_unk -verbose -beam_size 5 -dynamic_dict \
  -model model_data/
  23000
  0.39790565296191016
  0.2447908713266875
  0.169983296991242
  0.12287376129335324
  28000
  0.3895400113991531
  0.2405488552037889
  0.16799569624243538
  0.12233669719534132
}

カバレッジ
{
  精度はあまり上がらず
  23000
  0.39283195831915246
  0.23084639972780938
  0.154620030493061
  0.10791924631629284
  38000
  0.400871126394695
  0.23773701939324246
  0.16041060986342198
  0.11284668453882621
}

コピー+カバレッジ
{
  やはり、start_decay_stepsを早めていいと思う
  12000か15000ぐらい->精度は落ちた18000かむしろ遅くするか
  python preprocess.py \
  -train_src data/squad-src-train-du.txt \
  -train_tgt data/squad-tgt-train-du.txt \
  -valid_src data/squad-src-val-du.txt \
  -valid_tgt data/squad-tgt-val-du.txt \
  -save_data data/demo \
  -lower -dynamic_dict

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

  python translate.py \
  -src data/squad-src-val-du.txt \
  -output pred.txt \
  -replace_unk -verbose -dynamic_dict -beam_size 3 \
  -model model_data/

  27000(beam_size 5では動かなかった)
  0.42733110470325303
  0.26439422848537736
  0.18348144160902338
  0.1322294323402269
  46000(beamsize 2 batchsize 8)
  0.43659683558324947
  0.2683712796769928
  0.1850335441619655
  0.13277520076621513

  -start_decay_stepsを15000に
  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 15000 -decay_steps 2500 -train_steps 60000
  18000
  0.3971011092050099
  0.24383965416712444
  0.16872484697115472
  0.12167743950682257
  30000
  0.39870175776821487
  0.24607882917164436
  0.17144628693593253
  0.12455070037165465
}

duらのものでない、squad-src-train.txtで学習
{
  valデータがシャッフルされており、同じ文が含まれていないため数値が低いと推測
  実際にシャッフルされてないデータを使用すると、bleuスコアは高く出た。
  また、beam_sizeは2>3>5である気がする。要検証
  python preprocess.py \
  -train_src data/squad-src-train.txt \
  -train_tgt data/squad-tgt-train.txt \
  -valid_src data/squad-src-val.txt \
  -valid_tgt data/squad-tgt-val.txt \
  -save_data data/demo \
  -lower -dynamic_dict
  squad-src-train.txt、データはシャッフル済み
  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
  -gpu_ranks 2 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

  python translate.py \
  -src data/squad-src-val.txt \
  -output pred.txt \
  -replace_unk -verbose -dynamic_dict \
  -model model_data/

  26000
  0.37422325030594705
  0.22619345479585512
  0.15657536618543771
  0.11378448087007803

  31000(batch_size 16,beam_size 2->beam_size 5だと実行できなかったため)
  0.4072758645575424
  0.24618911860940765
  0.16926746519966543
  0.12131234016752716
  beam_size 3
  0.392171360638032
  0.23807774793728795
  0.16477981256087937
  0.11940936920759035

  valをシャッフルしていないデータでテスト
  python translate.py \
  -src data/squad-src-val-nonshuffle.txt \
  -output pred.txt \
  -replace_unk -verbose -dynamic_dict \
  -model model_data/
  python bleu.py \
  -src data/squad-src-val-nonshuffle.txt \
  -tgt data/squad-tgt-val-nonshuffle.txt \
  -pred pred.txt

  demo-model_step_26000_ppl_16.9188_acc_48.6529.pt
  26000(-beam_size 5)
  0.4159896071557287
  0.2584923298909386
  0.18037140862163062
  0.13040524793848138

  31000(beam_size 3)
  0.4323910548338726
  0.26848483754117797
  0.18656143861070532
  0.13411561910927766
  31000(beam_size 2)
  0.4489726481188136
  0.27807114292270935
  0.19153013022779644
  0.13610185425093663
}

interro
{
  疑問詞をつけて実権
  trainデータはシャッフル済み、valデータはしていない（精度が高くでる？)
  srcもtgtもinterro用にデータを作成->片方だけシャッフルして合って正しくテストできなかったから
  単語ベクトルはfix->sepを考えるとどうするべきか
  コピー、カバレッジは使用


  python preprocess.py \
  -train_src data/squad-src-train-interro.txt \
  -train_tgt data/squad-tgt-train-interro.txt \
  -valid_src data/squad-src-val-interro.txt \
  -valid_tgt data/squad-tgt-val-interro.txt \
  -save_data data/demo \
  -lower -dynamic_dict

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

  python translate.py \
  -src data/squad-src-val-interro.txt \
  -output pred.txt \
  -replace_unk -verbose -dynamic_dict \
  -model model_data/  -beam_size 3 -batch_size 16

  23000(demo-model_step_23000_ppl_12.0205_acc_56.1697.pt)
  beam_size 2
  0.48649163814253005
  0.35413721820048794
  0.27647473481934515
  0.21892831261744478
  28000(demo-model_step_28000_ppl_12.0609_acc_56.5888.pt)
  beam_size 3
  0.48070181218777946
  0.35073380216473277
  0.27458077002009196
  0.21788886391189688
  beam_size 2
  0.4944721591349904
  0.35973257135095393
  0.28036900140580195
  0.22155248694637134
  31000
  beam_size 3
  0.49774443073103686
  0.3618993312880334
  0.2818080414470461
  0.22224794295059433

  単語ベクトルをfixしない->sepの存在
  -beam_size 2
  23000(55.72)
  0.493539243334691
  0.35757175171191047
  0.27685162092823346
  0.21715738297732412
  28000(55.72)
  0.4950799420857694
  0.35788744979773607
  0.2774005830650735
  0.2181789126183156
  -beam_size 3
  0.4822448436960576
  0.34879010038433017
  0.27082900686586936
  0.21371481543112505
  -beam_size 4
  0.4731010971112809
  0.34286701001306785
  0.26677488744029193
  0.21105572266073877
  -beam_size 1
  0.3614844808427078
  0.24965648336130788
  0.18625813585364906
  0.14079848657367153

}

疑問詞について
{
  srcを複数、疑問詞なし
  python bleu_noninterro.py \
  -src data/squad-src-val-nonshuffle.txt \
  -tgt data/squad-tgt-val-nonshuffle.txt \
  -pred data/squad-pred-val-nonshuffle.txt \
  -noninterro data/squad-noninterro-val-interro.txt
  （疑問詞抜きのbleu）
  0.4274382950603582
  0.26312441639729117
  0.18004027730633437
  0.12872794521360065

  （疑問詞こみの普通のbleu)
  これがbleu.pyの実行結果と一致する
  0.4489726481188136
  0.27807114292270935
  0.19153013022779644
  0.13610185425093663

  srcを複数、疑問詞あり
  python bleu_noninterro.py \
  -src data/squad-src-val-nonshuffle.txt \
  -tgt data/squad-tgt-val-nonshuffle.txt \
  -pred data/squad-pred-val-interro.txt \
  -noninterro data/squad-noninterro-val-interro.txt

  demo-model_step_28000_ppl_12.0609_acc_56.5888.pt
  0.4388107007341951
  0.2803067322706341
  0.19612238948168204
  0.14307965659637892

  これがbleu.pyの実行結果と一致する
  0.540069243279377
  0.392997545174506
  0.30406491822793263
  0.2385764967357339

  srcはそれぞれ、疑問詞なし
  python bleu_noninterro.py \
  -src data/squad-src-val-interro.txt \
  -tgt data/squad-tgt-val-nonshuffle.txt \
  -pred data/squad-pred-val-nonshuffle.txt \
  -noninterro data/squad-noninterro-val-interro.txt
  0.36406168556973145
  0.2098442719219568
  0.13881712641852068
  0.09684457575000847

  0.37580796897424457
  0.21795695640238852
  0.14486546737604616
  0.10043371658269439

  srcはそれぞれ、疑問詞あり
  python bleu_noninterro.py \
  -src data/squad-src-val-interro.txt \
  -tgt data/squad-tgt-val-nonshuffle.txt \
  -pred data/squad-pred-val-interro.txt \
  -noninterro data/squad-noninterro-val-interro.txt
  0.3836153721513976
  0.23693433163324804
  0.16438270957056633
  0.12014678882373847

  0.49384338632525876
  0.3591639173518169
  0.27985258420589904
  0.22106952897274582

  考察
  src複数からそれぞれ（疑問詞ごとに分ける）すると性能は落ちる（当然）。2-3ポイントほど
  疑問詞をのぞいてbleuをすると、疑問詞なしで学習したものは精度が1ポイントほど落ちる
  疑問詞ありで学習したものは、精度が9,10ポイント程度落ちる。確実に当てられる疑問詞が無視されるから。

}

コピー、カバレッジなし
{
  interroあり
  python preprocess.py \
  -train_src data/squad-src-train-interro.txt \
  -train_tgt data/squad-tgt-train-interro.txt \
  -valid_src data/squad-src-val-interro.txt \
  -valid_tgt data/squad-tgt-val-interro.txt \
  -save_data data/demo \
  -lower

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

  python translate.py \
  -src data/squad-src-val-interro.txt \
  -output pred.txt \
  -replace_unk \
  -model model_data/

  demo-model_step_33000_ppl_17.7369_acc_54.5408.pt
  0.4614979957196342
  0.32843613105306757
  0.2514502371805991
  0.19476330474131034

  demo-model_step_51000_ppl_17.9567_acc_54.6382.pt
  0.4614979957196342
  0.32843613105306757
  0.2514502371805991
  0.19476330474131034

}

transformerで実験
answerを付け加えて実験
