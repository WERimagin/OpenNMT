文そのまま（答えや疑問詞など含めず）
質問文の文字数が5以下のノイズデータは省いたが、それ以外の文は全て使用した。
50文字までを訓練に使用

全ての文
0.40700896414178056
0.23134561150951782
0.15074457485064752
0.10203992991167184

Duらの論文を参考に、内容語の重複がないものを取り除く
疑問詞がないものも取り除く
{
  val:10570
  9737
  train:87599
  78685
  6000
  0.36869636616658863
  0.2063775967570529
  0.133824553207302
  0.09055310119681877
  7000
  0.3640534042527387
  0.2024536444218513
  0.13069366812041094
  0.08922714728875593
}


-overlap:オーバラップしていないものも含む
{
  val:10570
  10024
  train:87599
  81753
  0.35490593637131324
  0.19247020246941443
  0.12223150480960106
  0.08206431976612458
  -beam_size 3
  0.36389284042078746
  0.19556191563829126
  0.12306557484710942
  0.08141793460926491
}

-noninterro:疑問詞がないものも含む
{
  val:10570
  10270
  train:87599
  84365
  9500
  0.3689061536558604
  0.20711415393632365
  0.13572319383764175
  0.09340830219864989
  8500
  0.3605848002215529
  0.20136165336130474
  0.13075139460792767
  0.08934717916889375
  10000
  0.3482898660557566
  0.19104165741696685
  0.12325646936848747
  0.08324030642379629
  13000
  0.3546727016323772
  0.19729206924058848
  0.12703690987263444
  0.08592545606260674
}

コピーとカバレッジの追加
{
  5500
  0.36761840892680653
  0.22192467849212252
  0.154679173629004
  0.11291541501663589
}

duらのデータを使用
{
  5500
  0.37964762979441496
  0.21070869737860146
  0.1360045702569303
  0.09183197851379975
  7000
  0.39143732009863264
  0.22211157468158665
  0.14478863115254467
  0.0987884300592426

  enc_word_size 45000,dec 28000
  word_vec fix
  9000
  0.37964762979441496
  0.21070869737860146
  0.1360045702569303
  0.09183197851379975
}

疑問詞付きで実験
{
  <SEP> + 疑問詞で実験
  inputの辞書サイズは45000,outputは28000
  単語ベクトルは固定
  コピー、カバレッジはなし
  python preprocess.py \
  -train_src data/squad-src-train-interro.txt \
  -train_tgt data/squad-tgt-train.txt \
  -valid_src data/squad-src-val-interro.txt \
  -valid_tgt data/squad-tgt-val.txt \
  -save_data data/demo \
  -lower -src_vocab_size 45000 -tgt_vocab_size 28000

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -gpu_ranks 3 -world_size 1
  12000
  0.42352199549253094
  0.29244977053327637
  0.21881937859302225
  0.16631600370337762
  9000
  0.4365445602660493
  0.30249012971662864
  0.22645192962571192
  0.17202424167198602
}

src_seq_sizeを100に変更し、データ数を増やした
{
  duらのデータを使用
  ここから、src_seq_size 100,tgt_seq_size 50、enc_voc_size 45000,dec_vocab_size 28000に固定
  データサイズ:62000->70000
  python preprocess.py \
  -train_src data/squad-src-train-du.txt \
  -train_tgt data/squad-tgt-train-du.txt \
  -valid_src data/squad-src-val-du.txt \
  -valid_tgt data/squad-tgt-val-du.txt \
  -save_data data/demo \
  -lower

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -gpu_ranks 3 -world_size 1

  12000
  0.3818032773669874
  0.22207359215281677
  0.14836247284785528
  0.1038565408658504
  10000
  0.3797911121140998
  0.21980389442712892
  0.1454957579114939
  0.10071842183474895
}

全てのデータ
{
  overlap+noninterro
  sgdを使用
  python preprocess.py \
  -train_src data/squad-src-train-overlap-noninterro.txt \
  -train_tgt data/squad-tgt-train-overlap-noninterro.txt \
  -valid_src data/squad-src-val-overlap-noninterro.txt \
  -valid_tgt data/squad-tgt-val-overlap-noninterro.txt \
  -save_data data/demo \
  -lower \
  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 10000 -decay_steps 2000

  --beam_size 3
  18000
  0.37134221386631916
  0.2068268052269781
  0.13353502535049772
  0.09116215926940126
  -beam_size 5
  0.35703256254970045
  0.19998870428721507
  0.1303280621388004
  0.08979972747923132
  20500
  0.35238165102010544
  0.19838990179457527
  0.13000550458808965
  0.08987226052408974
}

shuffle
{
  データをpreprocessでシャッフル
  データはoverlap-noninterro
  13000
  0.35449933197783856
  0.19930672435560826
  0.13064106789167962
  0.09046163460412371

  duら
  stepsを25000に
  18000
  0.36857942154151785
  0.21345661038425523
  0.14225483513420595
  0.09894156819452919
  15000
  0.3830418887721869
  0.2242326172442351
  0.15009308209802488
  0.10519081126455221
  13000
  0.38603452563406127
  0.2231796731017263
  0.14824092517212442
  0.10281374896930358

  -start_decay_steps 20000 -decay_steps 2500 -train_steps 50000
  28000
  0.3988480580669166
  0.23560276332121058
  0.1585135752728077
  0.1114058235070068
  37000
  0.39658456174426027
  0.2358171198938562
  0.16000026399319953
  0.11342067478486642
  38000
  0.39549555475888737
  0.2352811643317181
  0.15972696221112345
  0.11339629816619744

  -start_decay_steps 20000 -decay_steps 3000 -train_steps 60000
  30000
  0.400140624537702
  0.23731986280714962
  0.1601483527090614
  0.1127673503484243
  41000
  0.40016893254925556
  0.23866364072141835
  0.16182145550580126
  0.11443930570133387
  qgevalcap/eval.py
  Bleu_1: 0.41161
  Bleu_2: 0.24549
  Bleu_3: 0.16618
  Bleu_4: 0.11771

  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2000 -train_steps 60000
  27000
  0.3935429317536642
  0.23440159835834948
  0.1591811646641791
  0.11273642282453042
  30000
  0.3967899897331695
  0.2362346318980176
  0.1599417471152941
  0.11286157890117823
}

gru
{
  gruを使用、sgdは相性が悪いため、adamを使用
  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -rnn_type GRU \
  -gpu_ranks 3 -world_size 1 \
  -train_steps 30000
  スコアはとても低い
}

bridge
{
  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -gpu_ranks 3 -world_size 1 \
  -bridge \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 3000 -train_steps 50000
  26000
  0.38182877287214717
  0.22205685359382502
  0.14773142620211194
  0.10315436253715742
  44000
  0.3953961968128539
  0.2313639103813019
  0.15466326394909802
  0.1078724355523195
}

コピー
{
  コピーのみを使用
  データはduらのもの
  収束が早い気がするので、-start_decay_stepsを小さくしてもいいきがする。
  python preprocess.py \
  -train_src data/squad-src-train-du.txt \
  -train_tgt data/squad-tgt-train-du.txt \
  -valid_src data/squad-src-val-du.txt \
  -valid_tgt data/squad-tgt-val-du.txt \
  -save_data data/demo \
  -lower -dynamic_dict

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

  python translate.py \
  -src data/squad-src-val-du.txt \
  -output pred.txt \
  -replace_unk -verbose -beam_size 5 -dynamic_dict \
  -model model_data/
  23000
  0.39790565296191016
  0.2447908713266875
  0.169983296991242
  0.12287376129335324
  28000
  0.3895400113991531
  0.2405488552037889
  0.16799569624243538
  0.12233669719534132
}

カバレッジ
{
  精度はあまり上がらず
  23000
  0.39283195831915246
  0.23084639972780938
  0.154620030493061
  0.10791924631629284
  38000
  0.400871126394695
  0.23773701939324246
  0.16041060986342198
  0.11284668453882621
}

コピー+カバレッジ
{
  やはり、start_decay_stepsを早めていいと思う
  12000か15000ぐらい->精度は落ちた18000かむしろ遅くするか
  python preprocess.py \
  -train_src data/squad-src-train-du.txt \
  -train_tgt data/squad-tgt-train-du.txt \
  -valid_src data/squad-src-val-du.txt \
  -valid_tgt data/squad-tgt-val-du.txt \
  -save_data data/demo \
  -lower -dynamic_dict

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

  python translate.py \
  -src data/squad-src-val-du.txt \
  -output pred.txt \
  -replace_unk -verbose -dynamic_dict -beam_size 3 \
  -model model_data/

  27000(beam_size 5では動かなかった)
  0.42733110470325303
  0.26439422848537736
  0.18348144160902338
  0.1322294323402269
  46000(beamsize 2 batchsize 8)
  0.43659683558324947
  0.2683712796769928
  0.1850335441619655
  0.13277520076621513

  -start_decay_stepsを15000に
  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 15000 -decay_steps 2500 -train_steps 60000
  18000
  0.3971011092050099
  0.24383965416712444
  0.16872484697115472
  0.12167743950682257
  30000
  0.39870175776821487
  0.24607882917164436
  0.17144628693593253
  0.12455070037165465
}

duらのものでない、squad-src-train.txtで学習
{
  valデータがシャッフルされており、同じ文が含まれていないため数値が低いと推測
  実際にシャッフルされてないデータを使用すると、bleuスコアは高く出た。
  また、beam_sizeは2>3>5である気がする。要検証
  python preprocess.py \
  -train_src data/squad-src-train.txt \
  -train_tgt data/squad-tgt-train.txt \
  -valid_src data/squad-src-val.txt \
  -valid_tgt data/squad-tgt-val.txt \
  -save_data data/demo \
  -lower -dynamic_dict
  squad-src-train.txt、データはシャッフル済み
  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
  -gpu_ranks 2 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

  python translate.py \
  -src data/squad-src-val.txt \
  -output pred.txt \
  -replace_unk -verbose -dynamic_dict \
  -model model_data/

  26000
  0.37422325030594705
  0.22619345479585512
  0.15657536618543771
  0.11378448087007803

  31000(batch_size 16,beam_size 2->beam_size 5だと実行できなかったため)
  0.4072758645575424
  0.24618911860940765
  0.16926746519966543
  0.12131234016752716
  beam_size 3
  0.392171360638032
  0.23807774793728795
  0.16477981256087937
  0.11940936920759035

  valをシャッフルしていないデータでテスト
  python translate.py \
  -src data/squad-src-val-nonshuffle.txt \
  -output pred.txt \
  -replace_unk -verbose -dynamic_dict \
  -model model_data/
  python bleu.py \
  -src data/squad-src-val-nonshuffle.txt \
  -tgt data/squad-tgt-val-nonshuffle.txt \
  -pred pred.txt

  demo-model_step_26000_ppl_16.9188_acc_48.6529.pt
  26000(-beam_size 5)
  0.4159896071557287
  0.2584923298909386
  0.18037140862163062
  0.13040524793848138

  31000(beam_size 3)
  0.4323910548338726
  0.26848483754117797
  0.18656143861070532
  0.13411561910927766
  31000(beam_size 2)
  0.4489726481188136
  0.27807114292270935
  0.19153013022779644
  0.13610185425093663
}

interro
{
  疑問詞をつけて実権
  trainデータはシャッフル済み、valデータはしていない（精度が高くでる？)
  srcもtgtもinterro用にデータを作成->片方だけシャッフルして合って正しくテストできなかったから
  単語ベクトルはfix->sepを考えるとどうするべきか
  コピー、カバレッジは使用


  python preprocess.py \
  -train_src data/squad-src-train-interro.txt \
  -train_tgt data/squad-tgt-train-interro.txt \
  -valid_src data/squad-src-val-interro.txt \
  -valid_tgt data/squad-tgt-val-interro.txt \
  -save_data data/demo \
  -lower -dynamic_dict

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

  python translate.py \
  -src data/squad-src-val-interro.txt \
  -output pred.txt \
  -replace_unk -verbose -dynamic_dict \
  -model model_data/  -beam_size 3 -batch_size 16

  23000(demo-model_step_23000_ppl_12.0205_acc_56.1697.pt)
  beam_size 2
  0.48649163814253005
  0.35413721820048794
  0.27647473481934515
  0.21892831261744478

  28000(demo-model_step_28000_ppl_12.0609_acc_56.5888.pt)
  beam_size 3
  0.48070181218777946
  0.35073380216473277
  0.27458077002009196
  0.21788886391189688
  beam_size 2
  0.4944721591349904
  0.35973257135095393
  0.28036900140580195
  0.22155248694637134
  31000
  beam_size 3
  0.49774443073103686
  0.3618993312880334
  0.2818080414470461
  0.22224794295059433

  単語ベクトルをfixしない->sepの存在
  -beam_size 2
  23000(55.72)
  0.493539243334691
  0.35757175171191047
  0.27685162092823346
  0.21715738297732412
  28000(55.72)
  0.4950799420857694
  0.35788744979773607
  0.2774005830650735
  0.2181789126183156
  -beam_size 3
  0.4822448436960576
  0.34879010038433017
  0.27082900686586936
  0.21371481543112505
  -beam_size 4
  0.4731010971112809
  0.34286701001306785
  0.26677488744029193
  0.21105572266073877
  -beam_size 1
  0.3614844808427078
  0.24965648336130788
  0.18625813585364906
  0.14079848657367153

}

疑問詞について
{
  srcを複数、疑問詞なし
  python bleu_noninterro.py \
  -src data/squad-src-val-nonshuffle.txt \
  -tgt data/squad-tgt-val-nonshuffle.txt \
  -pred data/squad-pred-val-nonshuffle.txt \
  -noninterro data/squad-noninterro-val-interro.txt

  demo-model_step_31000_ppl_17.0881_acc_48.7707.pt
  （疑問詞抜きのbleu）
  0.4274382950603582
  0.26312441639729117
  0.18004027730633437
  0.12872794521360065

  （疑問詞こみの普通のbleu)
  これがbleu.pyの実行結果と一致する
  0.4489726481188136
  0.27807114292270935
  0.19153013022779644
  0.13610185425093663

  srcを複数、疑問詞あり
  python bleu_noninterro.py \
  -src data/squad-src-val-nonshuffle.txt \
  -tgt data/squad-tgt-val-nonshuffle.txt \
  -pred data/squad-pred-val-interro.txt \
  -noninterro data/squad-noninterro-val-interro.txt

  demo-model_step_31000_ppl_17.0881_acc_48.7707.pt
  0.4388107007341951
  0.2803067322706341
  0.19612238948168204
  0.14307965659637892

  これがbleu.pyの実行結果と一致する
  0.540069243279377
  0.392997545174506
  0.30406491822793263
  0.2385764967357339

  srcはそれぞれ、疑問詞なし
  python bleu_noninterro.py \
  -src data/squad-src-val-interro.txt \
  -tgt data/squad-tgt-val-nonshuffle.txt \
  -pred data/squad-pred-val-nonshuffle.txt \
  -noninterro data/squad-noninterro-val-interro.txt

 demo-model_step_31000_ppl_17.0881_acc_48.7707.pt
  --beam_size 5
  0.326583001256204
  0.18901396654966804
  0.12790261482470383
  0.09153840207135153

  0.3395153796802409
  0.1974370402231964
  0.13308626661932121
  0.09432631366591678

  --beamsize 2
  demo-model_step_31000_ppl_17.0881_acc_48.7707.pt
  0.36406168556973145
  0.2098442719219568
  0.13881712641852068
  0.09684457575000847

  0.37580796897424457
  0.21795695640238852
  0.14486546737604616
  0.10043371658269439

  srcはそれぞれ、疑問詞あり
  python bleu_noninterro.py \
  -src data/squad-src-val-interro.txt \
  -tgt data/squad-tgt-val-nonshuffle.txt \
  -pred data/squad-pred-val-interro-beam5.txt \
  -noninterro data/squad-noninterro-val-interro.txt

  対象のぶんd絵はなく、前後の文を使用する。対象の文を含める。精度比較
  質問応答ではない
  質問生成としてどう評価するか
    疑問詞を入れる意味、どういう位置付け
    データセットの拡充という意味ではあまり意味がない。もう溶けるから

  機械に解けない質問文
  答えを入れた時の精度の変化（疑問詞のあと、疑問詞抜き）
  複数の文を見ないと質問文
  multi hop,複数文





  -beam 5
  0.3453830522531623
  0.2138323599599459
  0.14999690019303585
  0.11079939388827552

  0.46221160269487405
  0.3379914491279306
  0.26533754222924094
  0.21132804762887195

  考察
  src複数からそれぞれ（疑問詞ごとに分ける）すると性能は落ちる（当然）。2-3ポイントほど
  疑問詞をのぞいてbleuをすると、疑問詞なしで学習したものは精度が1ポイントほど落ちる
  疑問詞ありで学習したものは、精度が9,10ポイント程度落ちる。確実に当てられる疑問詞が無視されるから。

}

コピー、カバレッジなし
{
  interroあり
  python preprocess.py \
  -train_src data/squad-src-train-interro.txt \
  -train_tgt data/squad-tgt-train-interro.txt \
  -valid_src data/squad-src-val-interro.txt \
  -valid_tgt data/squad-tgt-val-interro.txt \
  -save_data data/demo \
  -lower

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

  python translate.py \
  -src data/squad-src-val-interro.txt \
  -output pred.txt \
  -replace_unk \
  -model model_data/

  demo-model_step_33000_ppl_17.7369_acc_54.5408.pt
  0.3474318307866521
  0.20531740524195985
  0.13954556980631552
  0.10081358661483941

  0.4614979957196342
  0.32843613105306757
  0.2514502371805991
  0.19476330474131034

  demo-model_step_51000_ppl_17.9567_acc_54.6382.pt
  0.4614979957196342
  0.32843613105306757
  0.2514502371805991
  0.19476330474131034

}

beam_sizeの比較
{
  srcを独立
  python bleu.py \
  -src data/squad-src-val-interro.txt \
  -tgt data/squad-tgt-val-interro.txt \
  -pred

  beam2>beam3>beam4>beam5
  beamsize 2
  0.4944721591349904
  0.35973257135095393
  0.28036900140580195
  0.22155248694637134

  beamsize 3
  0.48129937749301566
  0.35136360784180587
  0.27520103779499266
  0.21842927908880047

  beamsize 4
  0.470650229502534
  0.3442926937568763
  0.2701346050578339
  0.21471708153789001

  beamsize 5
  0.46221160269487405
  0.3379914491279306
  0.26533754222924094
  0.21132804762887195

  srcを複数
  beam2>beam3>beam4>beam5
  beam2
  0.5410323278253051
  0.3937096998518577
  0.30460095516052427
  0.23902662531129767
  beam3
  0.5278998877405486
  0.38602804960218823
  0.30058881870173815
  0.23737932616896285
  beam4
  0.5187402308309185
  0.3803790257315261
  0.29685336102625887
  0.2346970290299284
  beam5
  0.5097870107950786
  0.37397685388194674
  0.29208623829384267
  0.23151677862741574

  疑問詞抜き,srcはそれぞれ
  python bleu_noninterro.py \
  -src data/squad-src-val-interro.txt \
  -tgt data/squad-tgt-val-interro.txt \
  -noninterro data/squad-noninterro-val-interro.txt \
  -pred

  beamsize 2
  0.3842941768963818
  0.2374970827256178
  0.1648299857399002
  0.1204635874495911

  0.4944721591349904
  0.35973257135095393
  0.28036900140580195
  0.22155248694637134

  beamsize 5
  0.3453830522531623
  0.2138323599599459
  0.14999690019303585
  0.11079939388827552

  0.46221160269487405
  0.3379914491279306
  0.26533754222924094
  0.21132804762887195

}

やり直し
{
  疑問詞の?マークを取り除く
  the,the,...のように連続したものを除去
  sentenceの分割をnltkに変更
  この条件でinterroを使った質問生成
  fixを設定で忘れていたので追加
  python preprocess.py \
  -train_src data/squad-src-train-interro.txt \
  -train_tgt data/squad-tgt-train-interro.txt \
  -valid_src data/squad-src-val-interro.txt \
  -valid_tgt data/squad-tgt-val-interro.txt \
  -save_data data/demo \
  -lower -dynamic_dict

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

  python translate.py \
  -src data/squad-src-val-interro.txt \
  -output pred.txt \
  -replace_unk -dynamic_dict\
  -length_penalty avg \
  -model model_data/

  python bleu.py \
  -src data/squad-src-val-interro.txt \
  -tgt data/squad-tgt-val-interro.txt \
  -pred pred.txt

  python bleu_noninterro.py \
  -pred pred.txt

  demo-model_step_33000_ppl_11.9292_acc_56.7303.pt
  0.3975609908906012
  0.24891312588288864
  0.17457407327874952
  0.1284325392444405

  0.5045372750880516
  0.36847654057226176
  0.28806625485724674
  0.2286262058783443


  demo-model_step_28000_ppl_11.8541_acc_56.5289.pt
  0.49773664346497004
  0.3628009541249536
  0.28349837368466735
  0.22499884309669207

  --length_penalty wu
  0.49773664346497004
  0.3628009541249536
  0.28349837368466735
  0.22499884309669207

  --length_penalty avg
  0.5025626991443817
  0.36655626286980736
  0.28653498494945284
  0.22744561610146702

  beam5 avg
  0.4832065868870641
  0.35406794689197385
  0.278156030105797
  0.22173113292896543



  demo-model_step_33000_ppl_11.9292_acc_56.7303.pt
  --length_penalty avg
  0.5045372750880516
  0.36847654057226176
  0.28806625485724674
  0.2286262058783443

  no length penalty
  0.4997517790850251
  0.3652903283501461
  0.2858435187520519
  0.2271285164733917

  --length_penalty wu
  0.4997517790850251
  0.3652903283501461
  0.2858435187520519
  0.2271285164733917


}

  interroなし
{
  python preprocess.py \
  -train_src data/squad-src-train-normal.txt \
  -train_tgt data/squad-tgt-train-normal.txt \
  -valid_src data/squad-src-val-normal.txt \
  -valid_tgt data/squad-tgt-val-normal.txt \
  -save_data data/demo \
  -lower -dynamic_dict

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000

  python translate.py \
  -src data/squad-src-val-interro.txt \
  -output pred.txt \
  -replace_unk -dynamic_dict -length_penalty avg \
  -model model_data/

  python translate.py \
  -src data/squad-src-train-full-sentence.txt \
  -output pred.txt \
  -replace_unk -dynamic_dict -length_penalty avg \
  -model model_data/demo-model_step_36000_ppl_17.4393_acc_48.5345.pt


  demo-model_step_36000_ppl_17.4393_acc_48.5345.pt
  0.3955432751381806
  0.23620602659204437
  0.1614321699565451
  0.1150910279724184
  squad-pred-val-normal.txt

  -length_penaltyなし
  0.38879295746827147
  0.2313044301788705
  0.1575652632014402
  0.11217150622436259

  demo-model_step_30000_ppl_17.4093_acc_48.4701.pt
  0.3958691272103621
  0.23529535212788663
  0.15995035223218648
  0.11398689189422896
}

normal,コピー、カバレッジなし
{
  python preprocess.py \
  -train_src data/squad-src-train-normal.txt \
  -train_tgt data/squad-tgt-train-normal.txt \
  -valid_src data/squad-src-val-normal.txt \
  -valid_tgt data/squad-tgt-val-normal.txt \
  -save_data data/demo \
  -lower

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -gpu_ranks 3 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 50000
}

interro、コピーカバレッジなし
{
  python preprocess.py \
  -train_src data/squad-src-train-interro.txt \
  -train_tgt data/squad-tgt-train-interro.txt \
  -valid_src data/squad-src-val-interro.txt \
  -valid_tgt data/squad-tgt-val-interro.txt \
  -save_data data/demo \
  -lower

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -gpu_ranks 2 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 60000
}

pre,be
{
  一つ前の文も使用
  (ただしコピー機構を忘れてるのでやり直し)
  python preprocess.py \
  -train_src data/squad-src-train-interro-pre.txt \
  -train_tgt data/squad-tgt-train-interro-pre.txt \
  -valid_src data/squad-src-val-interro-pre.txt \
  -valid_tgt data/squad-tgt-val-interro-pre.txt \
  -save_data data/demo \
  -lower -dynamic_dict

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python translate.py \
  -src data/squad-src-val-interro-pre.txt \
  -output pred.txt \
  -replace_unk -dynamic_dict \
  -length_penalty avg \
  -model model_data

  python bleu.py

  demo-model_step_28000_ppl_17.2756_acc_55.1112.pt
  0.4900451424710959
  0.3471740670309648
  0.2642918904800276
  0.20386143766335796

  demo-model_step_42000_ppl_17.6708_acc_55.3027.pt
  0.4919776546095722
  0.34939364366528347
  0.266987352717384
  0.20672488438778683

  一つ前の文のみ
  python preprocess.py \
  -train_src data/squad-src-train-interro-pre-nonsentence.txt \
  -train_tgt data/squad-tgt-train-interro-pre-nonsentence.txt \
  -valid_src data/squad-src-val-interro-pre-nonsentence.txt \
  -valid_tgt data/squad-tgt-val-interro-pre-nonsentence.txt \
  -save_data data/demo \
  -lower -dynamic_dict

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
  -gpu_ranks 1 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 50000

  python translate.py \
  -src data/squad-src-val-interro-pre-nonsentence.txt \
  -output pred.txt \
  -replace_unk -dynamic_dict \
  -length_penalty avg \
  -model model_data/

  demo-model_step_11000_ppl_34.2267_acc_45.3541.pt
  demo-model_step_18000_ppl_35.3044_acc_45.2018.pt
  demo-model_step_21000_ppl_35.8494_acc_45.6021.pt
  0.22679279410460595
  0.06450191440000551
  0.025404466937417075
  0.0114230497680811

  demo-model_step_28000_ppl_40.2296_acc_45.774.pt
  0.2261251072951863
  0.06235751743987969
  0.025079192676208593
  0.010942787149933606

  一つ前の文と本体の文を使用
  python preprocess.py \
  -train_src data/squad-src-train-interro-pre.txt \
  -train_tgt data/squad-tgt-train-interro-pre.txt \
  -valid_src data/squad-src-val-interro-pre.txt \
  -valid_tgt data/squad-tgt-val-interro-pre.txt \
  -save_data data/demo \
  -lower -dynamic_dict

  python embeddings_to_torch.py \
  -emb_file_both "../data/glove.840B.300d.txt" \
  -dict_file "data/demo.vocab.pt" \
  -output_file "data/embeddings"

  python3 train.py \
  -data data/demo \
  -save_model model_data/demo-model \
  -pre_word_vecs_enc "data/embeddings.enc.pt" -pre_word_vecs_dec "data/embeddings.dec.pt" \
  -fix_word_vecs_enc -fix_word_vecs_dec \
  -copy_attn -copy_attn_type general -coverage_attn -lambda_coverage 1 \
  -gpu_ranks 1 -world_size 1 \
  -optim sgd -learning_rate 1 -learning_rate_decay 0.5 -start_decay_steps 20000 -decay_steps 2500 -train_steps 50000
}

transformerで実験
answerを付け加えて実験
... the the...のように単語が重複している部分を取り除く
